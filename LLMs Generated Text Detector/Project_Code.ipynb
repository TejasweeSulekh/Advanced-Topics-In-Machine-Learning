{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dill as pickle\n",
    "import tiktoken\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, mu=None, sigma=None, ret_mu_sigma=False):\n",
    "    \"\"\"\n",
    "    Normalizes data, where data is a matrix where the first dimension is the number of examples\n",
    "    \"\"\"\n",
    "    if mu is None:\n",
    "        mu = np.mean(data, axis=0)\n",
    "    if sigma is None:\n",
    "        raw_std = np.std(data, axis=0)\n",
    "        sigma = np.ones_like(raw_std)\n",
    "        sigma[raw_std != 0] = raw_std[raw_std != 0]\n",
    "\n",
    "    if ret_mu_sigma:\n",
    "        return (data - mu) / sigma, mu, sigma\n",
    "    else:\n",
    "        return (data - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_len(tokens):\n",
    "    \"\"\"\n",
    "    Returns a vector of word lengths, in tokens\n",
    "    \"\"\"\n",
    "    tokens_len = []\n",
    "    curr = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token[0] == \"Ġ\":\n",
    "            tokens_len.append(curr)\n",
    "            curr = 1\n",
    "        else:\n",
    "            curr += 1\n",
    "\n",
    "    return np.array(tokens_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ngram(doc, model, tokenizer, n=3, strip_first=False):\n",
    "    \"\"\"\n",
    "    Returns vector of ngram probabilities given document, model and tokenizer\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    if strip_first:\n",
    "        doc = \" \".join(doc.split()[:1000])\n",
    "    for i in ngrams((n - 1) * [50256] + tokenizer(doc.strip()), n):\n",
    "        scores.append(model.n_gram_probability(i))\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    \"\"\"\n",
    "    An n-gram model, where alpha is the laplace smoothing parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_text, n=2, alpha=3e-3, vocab_size=None):\n",
    "        self.n = n\n",
    "        if vocab_size is None:\n",
    "            # Assume GPT tokenizer\n",
    "            self.vocab_size = 50257\n",
    "\n",
    "        self.smoothing = alpha\n",
    "        self.smoothing_f = alpha * self.vocab_size\n",
    "\n",
    "        self.c = defaultdict(lambda: [0, Counter()])\n",
    "        for i in tqdm.tqdm(range(len(train_text)-n)):\n",
    "            n_gram = tuple(train_text[i:i+n])\n",
    "            self.c[n_gram[:-1]][1][n_gram[-1]] += 1\n",
    "            self.c[n_gram[:-1]][0] += 1\n",
    "        self.n_size = len(self.c)\n",
    "\n",
    "    def n_gram_probability(self, n_gram):\n",
    "        assert len(n_gram) == self.n\n",
    "        it = self.c[tuple(n_gram[:-1])]\n",
    "        prob = (it[1][n_gram[-1]] + self.smoothing)/(it[0] + self.smoothing_f)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscountBackoffModel(NGramModel):\n",
    "    \"\"\"\n",
    "    An n-gram model with discounting and backoff. Delta is the discounting parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_text, lower_order_model, n=2, delta=0.9):\n",
    "        super().__init__(train_text, n=n)\n",
    "        self.lower_order_model = lower_order_model\n",
    "        self.discount = delta\n",
    "\n",
    "    def n_gram_probability(self, n_gram):\n",
    "        assert len(n_gram) == self.n\n",
    "        it = self.c[tuple(n_gram[:-1])]\n",
    "\n",
    "        if it[0] == 0:\n",
    "            return self.lower_order_model.n_gram_probability(n_gram[1:])\n",
    "\n",
    "        prob = self.discount * \\\n",
    "            (len(it[1])/it[0]) * \\\n",
    "            self.lower_order_model.n_gram_probability(n_gram[1:])\n",
    "        if it[1][n_gram[-1]] != 0:\n",
    "            prob += max(it[1][n_gram[-1]] - self.discount, 0) / it[0]\n",
    "\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneserNeyBaseModel(NGramModel):\n",
    "    \"\"\"\n",
    "    A Kneser-Ney base model, where n=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_text, vocab_size=None):\n",
    "        super().__init__(train_text, n=1, vocab_size=vocab_size)\n",
    "\n",
    "        base_cnt = defaultdict(set)\n",
    "        for i in range(1, len(train_text)):\n",
    "            base_cnt[train_text[i]].add(train_text[i-1])\n",
    "\n",
    "        cnt = 0\n",
    "        for word in base_cnt:\n",
    "            cnt += len(base_cnt[word])\n",
    "\n",
    "        self.prob = defaultdict(float)\n",
    "        for word in base_cnt:\n",
    "            self.prob[word] = len(base_cnt[word]) / cnt\n",
    "\n",
    "    def n_gram_probability(self, n_gram):\n",
    "        assert len(n_gram) == 1\n",
    "        ret_prob = self.prob[n_gram[0]]\n",
    "\n",
    "        if ret_prob == 0:\n",
    "            return 1 / self.vocab_size\n",
    "        else:\n",
    "            return ret_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrigramBackoff:\n",
    "    \"\"\"\n",
    "    A trigram model with discounting and backoff. Uses a Kneser-Ney base model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_text, delta=0.9):\n",
    "        self.base = KneserNeyBaseModel(train_text)\n",
    "        self.bigram = DiscountBackoffModel(\n",
    "            train_text, self.base, n=2, delta=delta)\n",
    "        self.trigram = DiscountBackoffModel(\n",
    "            train_text, self.bigram, n=3, delta=delta)\n",
    "\n",
    "    def n_gram_probability(self, n_gram):\n",
    "        assert len(n_gram) == 3\n",
    "        return self.trigram.n_gram_probability(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trigram(verbose=True, return_tokenizer=False):\n",
    "    \"\"\"\n",
    "    Trains and returns a trigram model on the brown corpus\n",
    "    \"\"\"\n",
    "\n",
    "    enc = tiktoken.encoding_for_model(\"davinci\")\n",
    "    tokenizer = enc.encode\n",
    "    vocab_size = enc.n_vocab\n",
    "\n",
    "    # We use the brown corpus to train the n-gram model\n",
    "    sentences = brown.sents()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Tokenizing corpus...\")\n",
    "    tokenized_corpus = []\n",
    "    for sentence in tqdm.tqdm(sentences):\n",
    "        tokens = tokenizer(\" \".join(sentence))\n",
    "        tokenized_corpus += tokens\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nTraining n-gram model...\")\n",
    "\n",
    "    if return_tokenizer:\n",
    "        return TrigramBackoff(tokenized_corpus), tokenizer\n",
    "    else:\n",
    "        return TrigramBackoff(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(exp):\n",
    "    \"\"\"\n",
    "    Splits up expression into words, to be individually processed\n",
    "    \"\"\"\n",
    "    return exp.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "DIR_IGNORE = {\"logprobs\", \"prompts\", \"headlines\"}\n",
    "\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    type: str\n",
    "    path: str\n",
    "\n",
    "\n",
    "def get_generate_dataset_normal(path: str, verbose=False):\n",
    "    files = []\n",
    "    to_iter = tqdm.tqdm(os.listdir(path)) if verbose else os.listdir(path)\n",
    "\n",
    "    for file in to_iter:\n",
    "        if file in DIR_IGNORE:\n",
    "            continue\n",
    "        files.append(f\"{path}/{file}\")\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_generate_dataset_author(path: str, author: str, verbose=False):\n",
    "    files = []\n",
    "\n",
    "    if author is None:\n",
    "        authors = sorted(os.listdir(path))\n",
    "    else:\n",
    "        authors = [author]\n",
    "\n",
    "    to_iter = tqdm.tqdm(authors) if verbose else authors\n",
    "\n",
    "    for author in to_iter:\n",
    "        for file in sorted(os.listdir(f\"{path}/{author}\")):\n",
    "            if file in DIR_IGNORE:\n",
    "                continue\n",
    "            files.append(f\"{path}/{author}/{file}\")\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_generate_dataset(*datasets: Dataset):\n",
    "    def generate_dataset(featurize, split=None, verbose=False, author=None):\n",
    "        files = []\n",
    "        for dataset in datasets:\n",
    "            if dataset.type == \"normal\":\n",
    "                files += get_generate_dataset_normal(dataset.path)\n",
    "            elif dataset.type == \"author\":\n",
    "                files += get_generate_dataset_author(dataset.path, author=author)\n",
    "\n",
    "        if split is not None:\n",
    "            files = np.array(files)[split]\n",
    "\n",
    "        data = []\n",
    "        files = tqdm.tqdm(files) if verbose else files\n",
    "\n",
    "        for file in files:\n",
    "            data.append(featurize(file))\n",
    "        return np.array(data)\n",
    "\n",
    "    return generate_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_dataset = [\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\human\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\gpt\"),\n",
    "]\n",
    "\n",
    "reuter_dataset = [\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\human\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\gpt\"),\n",
    "]\n",
    "\n",
    "essay_dataset = [\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\human\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\gpt\"),\n",
    "]\n",
    "\n",
    "eval_dataset = [\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\claude\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\claude\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\claude\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\gpt_prompt1\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\gpt_prompt1\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\gpt_prompt1\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\gpt_prompt2\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\gpt_prompt2\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\gpt_prompt2\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\gpt_writing\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\gpt_writing\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\gpt_writing\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\wp\\gpt_semantic\"),\n",
    "    Dataset(\"author\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\reuter\\gpt_semantic\"),\n",
    "    Dataset(\"normal\", r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\gpt_semantic\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    *wp_dataset,\n",
    "    *reuter_dataset,\n",
    "    *essay_dataset,\n",
    "]\n",
    "author = None\n",
    "files = []\n",
    "for dataset in datasets:\n",
    "    if dataset.type == \"normal\":\n",
    "        files += get_generate_dataset_normal(dataset.path)\n",
    "    elif dataset.type == \"author\":\n",
    "        files += get_generate_dataset_author(dataset.path, author=author)\n",
    "\n",
    "    data = []\n",
    "    files = tqdm.tqdm(files) if False else files\n",
    "    for file in files:\n",
    "        data.append(featurize(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Stuff\\\\VScode\\\\Workspace\\\\Notebooks\\\\ATML\\\\Project\\\\data\\\\wp\\\\gpt/1.txt'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(files)\n",
    "files[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_logprobs(\n",
    "    generate_dataset,\n",
    "    preprocess=lambda x: x.strip(),\n",
    "    verbose=True,\n",
    "    trigram=None,\n",
    "    tokenizer=None,\n",
    "    num_tokens=2047,\n",
    "):\n",
    "    if trigram is None:\n",
    "        trigram, tokenizer = train_trigram(verbose=verbose, return_tokenizer=True)\n",
    "\n",
    "    # davinci_logprobs, ada_logprobs = {}, {}\n",
    "    trigram_logprobs, unigram_logprobs = {}, {}\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Loading logprobs into memory\")\n",
    "\n",
    "    file_names = generate_dataset(lambda file: file, verbose=False)\n",
    "    to_iter = tqdm.tqdm(file_names) if verbose else file_names\n",
    "\n",
    "    for file in to_iter:\n",
    "        with open(file, \"r\") as f:\n",
    "            doc = preprocess(f.read())\n",
    "        # davinci_logprobs[file] = get_logprobs(\n",
    "        #     convert_file_to_logprob_file(file, \"davinci\")\n",
    "        # )[:num_tokens]\n",
    "        # ada_logprobs[file] = get_logprobs(convert_file_to_logprob_file(file, \"ada\"))[\n",
    "            # :num_tokens\n",
    "        # ]\n",
    "        trigram_logprobs[file] = score_ngram(doc, trigram, tokenizer, n=3)[:num_tokens]\n",
    "        unigram_logprobs[file] = score_ngram(doc, trigram.base, tokenizer, n=1)[\n",
    "            :num_tokens\n",
    "        ]\n",
    "\n",
    "    # return davinci_logprobs, ada_logprobs, trigram_logprobs, unigram_logprobs\n",
    "    return trigram_logprobs, unigram_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_functions = {\n",
    "    \"v-add\": lambda a, b: a + b,\n",
    "    \"v-sub\": lambda a, b: a - b,\n",
    "    \"v-mul\": lambda a, b: a * b,\n",
    "    \"v-div\": lambda a, b: np.divide(\n",
    "        a, b, out=np.zeros_like(a), where=(b != 0), casting=\"unsafe\"\n",
    "    ),\n",
    "    \"v->\": lambda a, b: a > b,\n",
    "    \"v-<\": lambda a, b: a < b,\n",
    "}\n",
    "\n",
    "scalar_functions = {\n",
    "    \"s-max\": max,\n",
    "    \"s-min\": min,\n",
    "    \"s-avg\": lambda x: sum(x) / len(x),\n",
    "    \"s-avg-top-25\": lambda x: sum(sorted(x, reverse=True)[:25])\n",
    "    / len(sorted(x, reverse=True)[:25]),\n",
    "    \"s-len\": len,\n",
    "    \"s-var\": np.var,\n",
    "    \"s-l2\": np.linalg.norm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exp_featurize(best_features, vector_map):\n",
    "    def calc_features(file, exp):\n",
    "        exp_tokens = get_words(exp)\n",
    "        curr = vector_map[exp_tokens[0]](file)\n",
    "\n",
    "        for i in range(1, len(exp_tokens)):\n",
    "            if exp_tokens[i] in vec_functions:\n",
    "                next_vec = vector_map[exp_tokens[i + 1]](file)\n",
    "                curr = vec_functions[exp_tokens[i]](curr, next_vec)\n",
    "            elif exp_tokens[i] in scalar_functions:\n",
    "                return scalar_functions[exp_tokens[i]](curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_featurized_data(generate_dataset_fn, best_features):\n",
    "    # t_data = generate_dataset_fn(t_featurize)\n",
    "\n",
    "    trigram, unigram = get_all_logprobs(\n",
    "        generate_dataset_fn, trigram=trigram_model, tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    vector_map = {\n",
    "        # \"davinci-logprobs\": lambda file: davinci[file],\n",
    "        # \"ada-logprobs\": lambda file: ada[file],\n",
    "        \"trigram-logprobs\": lambda file: trigram[file],\n",
    "        \"unigram-logprobs\": lambda file: unigram[file],\n",
    "    }\n",
    "    exp_featurize = get_exp_featurize(best_features, vector_map)\n",
    "    exp_data = generate_dataset_fn(exp_featurize)\n",
    "\n",
    "    # return np.concatenate([t_data, exp_data], axis=1) This was removed\n",
    "    return np.concatenate([exp_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\data\\essay\\human\\4.txt\"\n",
    "MAX_TOKENS = 2048\n",
    "best_features = open(r\"D:\\Stuff\\VScode\\Workspace\\Notebooks\\ATML\\Project\\model\\features.txt\").read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "np.random.seed(1)\n",
    "\n",
    "result_table = [[\"F1\", \"Accuracy\", \"AUC\"]]\n",
    "\n",
    "datasets = [\n",
    "    *wp_dataset,\n",
    "    *reuter_dataset,\n",
    "    *essay_dataset,\n",
    "]\n",
    "generate_dataset_fn = get_generate_dataset(*datasets)\n",
    "\n",
    "labels = generate_dataset_fn(\n",
    "    lambda file: 1 if any([m in file for m in [\"gpt\", \"claude\"]]) else 0 # 0 => Human else 1 => Not Human\n",
    ")\n",
    "indices = np.arange(len(labels))\n",
    "np.random.shuffle(indices)\n",
    "train, test = (\n",
    "    indices[: math.floor(0.8 * len(indices))],\n",
    "    indices[math.floor(0.8 * len(indices)) :],\n",
    ")\n",
    "print(\"Train/Test Split\", train, test)\n",
    "print(\"Train Size:\", len(train), \"Valid Size:\", len(test))\n",
    "print(f\"Positive Labels: {sum(labels[indices])}, Total Labels: {len(indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tsule\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsule\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator _SigmoidCalibration from version 1.2.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\tsule\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:347: InconsistentVersionWarning: Trying to unpickle estimator CalibratedClassifierCV from version 1.2.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load davinci tokenizer\n",
    "enc = tiktoken.encoding_for_model(\"davinci\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Autumn is not generally viewed as an appropriate season for having a garage sale, not to mention that one on a scale of a local community. However, what makes my neighbourhood particularly unique and different from others is its consistent inability to meet expectations (Pyrkosz 147). Either due to some event that occurred too long ago for any of the neighbours to remember or care, or out of sheer need to have something extravagant in the midst of one of the least unpredictable seasons ever, my community has a massive garage sale every autumn. Although the weather and the lingering threat of health issues complicated the task, the community garage sale carried out last fall was one of the highlights of its members’ public life due to the opportunity to communicate and share memories, objects, and ideas.\n",
      "The range and diversity of the items that our numerous neighbours and community members offered as sales items could easily make one feel surprised. Though being admittedly cheap and often worn-out, these items told exciting stories about the lives of their owners (Hassan 68). For instance, some of the kitchenware sold during the event was quite old, with the style thereof allowing tracking down its origin to quite far back in the past. Though these items were certainly not of high price, they still led to rather fascinating revelations about the cultural legacy of their owners and their fascinating life stories.\n",
      "As it often happens during large garage sales, there was some drama. For instance, one of the neighborhood children recognized one of their treasured possessions among the sold items, causing quite a stir. Luckily, the buyer was understanding enough and returned the purchase, which turned out to be a collection of action figures, without any questions. It was truly heartwarming to observe the understanding and cooperation that buyers showed during that garage sale.\n",
      "Furthermore, it was worth noting that a substantial portion of the money collected during the garage sale was donated to assist those in need. Specifically, those suffering from the coronavirus, as well as members of impoverished communities and several local charities, were provided with a portion of the finances form the yard sale. Though the event resulted in a rather humble outcome, the money donated to the specified parties were definitely used to improve the lives of those suffering inequality and health disparities (Arasteh 867). Therefore, the yard sale was not only an important social event and a chance for the participants to earn some money, but also an opportunity to improve the lives of those in need.\n",
      "Apart from the economic benefit that the community garage sale introduced, the chance to share memories, ideas, and emotions was a particularly important part of the event. Though the concept of a yard sale might seem far too common and, therefore, lacking excitement, it, in fact, represents a unique opportunity to communicate and share experiences for the participants involved. Given the diversity of my community, the process was especially enlightening and leading to multiple new insights. Though the season might be seen as unsuitable for a yard sale, especially on a community scale, it di provide a perfect opportunity to embrace the opportunities that it provided and encourage participants to share their ideas. Thus, the weird yet lovely tradition of our community, the fall yard sale has left quite an impact on me.\n"
     ]
    }
   ],
   "source": [
    "# Load data and featurize\n",
    "with open(file) as f:\n",
    "    doc = f.read().strip()\n",
    "    # Strip data to first MAX_TOKENS tokens\n",
    "    tokens = enc.encode(doc)[:MAX_TOKENS]\n",
    "    doc = enc.decode(tokens).strip()\n",
    "\n",
    "    print(f\"Input: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')\n",
    "# Train trigram\n",
    "print(\"Loading Trigram...\")\n",
    "\n",
    "trigram_model = train_trigram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is for making a loop with features and their labels will be the indeces of labels variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\tsule\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: six in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tsule\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## helper function (nicer printing)\n",
    "##################################################\n",
    "\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 80 * '-')\n",
    "    print(textwrap.fill(tokenizer.decode(s, skip_special_tokens=True),80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model:  gpt2-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5210fe131876405caf594cf8ea1870ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.25G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca7ae72d4ba4d9dbf62213c4e098876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2-large\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1280,\n",
      "  \"n_head\": 20,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 36,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_scores\": true,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.32.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "## instantiating LLM & its tokenizer\n",
    "##################################################\n",
    "\n",
    "# model_to_use = \"gpt2\"\n",
    "model_to_use = \"gpt2-large\"\n",
    "\n",
    "print(\"Using model: \", model_to_use)\n",
    "\n",
    "# get the tokenizer for the pre-trained LM you would like to use\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_to_use)\n",
    "\n",
    "# instantiate a model (causal LM)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_to_use,\n",
    "                                        output_scores=True,\n",
    "                                        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# inspecting the (default) model configuration\n",
    "# (it is possible to created models with different configurations)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essay = open(\"4.txt\").read() # Machine generated\n",
    "input_tokens = tokenizer(essay, return_tensors=\"pt\").input_ids\n",
    "\n",
    "##################################################\n",
    "## retrieving next-word surprisals from GPT-2\n",
    "##################################################\n",
    "\n",
    "# NB: we can supply tensors of labels (token ids for next-words, no need to right-shift)\n",
    "# using -100 in the labels means: \"don't compute this one\"\n",
    "labels        = torch.clone(input_tokens)\n",
    "labels[0,0]   = -100\n",
    "output_word2  = model(input_tokens[:,0:2], labels= labels[:,0:2])\n",
    "output_prompt = model(input_tokens, labels=input_tokens)\n",
    "\n",
    "# negative log-likelihood of provided labels\n",
    "nll_word2  = output_word2.loss\n",
    "nll_output = output_prompt.loss * input_tokens.size(1)\n",
    "print(\"NLL of second word: \", nll_word2.item())\n",
    "print(\"NLL of whole output:\", nll_output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_exp_features = []\n",
    "final_train_labels = []\n",
    "for i in train:\n",
    "    # Load data and featurize\n",
    "    with open(files[i]) as f:\n",
    "        doc = f.read().strip()\n",
    "        # Strip data to first MAX_TOKENS tokens\n",
    "        tokens = enc.encode(doc)[:MAX_TOKENS]\n",
    "        doc = enc.decode(tokens).strip()\n",
    "    trigram_model = train_trigram()\n",
    "\n",
    "    trigram = np.array(score_ngram(doc, trigram_model, enc.encode, n=3, strip_first=False))\n",
    "    unigram = np.array(score_ngram(doc, trigram_model.base, enc.encode, n=1, strip_first=False))\n",
    "    \n",
    "    vector_map = {\n",
    "    # \"gpt-logprobs\": a,\n",
    "    \"trigram-logprobs\": trigram,\n",
    "    \"unigram-logprobs\": unigram\n",
    "    }\n",
    "    \n",
    "    exp_features = []\n",
    "    for exp in best_features:\n",
    "\n",
    "        exp_tokens = get_words(exp)\n",
    "        curr = vector_map[exp_tokens[0]]\n",
    "\n",
    "        for i in range(1, len(exp_tokens)):\n",
    "            if exp_tokens[i] in vec_functions:\n",
    "                next_vec = vector_map[exp_tokens[i+1]]\n",
    "                curr = vec_functions[exp_tokens[i]](curr, next_vec)\n",
    "            elif exp_tokens[i] in scalar_functions:\n",
    "                exp_features.append(scalar_functions[exp_tokens[i]](curr))\n",
    "                break\n",
    "    #Adding another feature from GPT-2\n",
    "    essay = open(files[i]).read() # Machine generated\n",
    "    input_tokens = tokenizer(essay, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    labels        = torch.clone(input_tokens)\n",
    "    labels[0,0]   = -100\n",
    "    output_prompt = model(input_tokens, labels=input_tokens)\n",
    "    nll_output = output_prompt.loss * input_tokens.size(1)\n",
    "    exp_features.append(nll_output)\n",
    "    final_train_exp_features.append(exp_features)\n",
    "    final_train_labels.append(labels[i])\n",
    "\n",
    "    # print(f\"Input: {doc}\")\n",
    "# files[train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_exp_features = []\n",
    "final_test_labels = []\n",
    "for i in test[:350]:\n",
    "    # Load data and featurize\n",
    "    with open(files[i]) as f:\n",
    "        doc = f.read().strip()\n",
    "        # Strip data to first MAX_TOKENS tokens\n",
    "        tokens = enc.encode(doc)[:MAX_TOKENS]\n",
    "        doc = enc.decode(tokens).strip()\n",
    "    # trigram_model = train_trigram()\n",
    "\n",
    "    trigram = np.array(score_ngram(doc, trigram_model, enc.encode, n=3, strip_first=False))\n",
    "    unigram = np.array(score_ngram(doc, trigram_model.base, enc.encode, n=1, strip_first=False))\n",
    "    \n",
    "    vector_map = {\n",
    "    # \"gpt-logprobs\": a,\n",
    "    \"trigram-logprobs\": trigram,\n",
    "    \"unigram-logprobs\": unigram\n",
    "    }\n",
    "    \n",
    "    exp_features = []\n",
    "    for exp in best_features:\n",
    "\n",
    "        exp_tokens = get_words(exp)\n",
    "        curr = vector_map[exp_tokens[0]]\n",
    "\n",
    "        for i in range(1, len(exp_tokens)):\n",
    "            if exp_tokens[i] in vec_functions:\n",
    "                next_vec = vector_map[exp_tokens[i+1]]\n",
    "                curr = vec_functions[exp_tokens[i]](curr, next_vec)\n",
    "            elif exp_tokens[i] in scalar_functions:\n",
    "                exp_features.append(scalar_functions[exp_tokens[i]](curr))\n",
    "                break\n",
    "            \n",
    "    #Adding another feature from GPT-2 the average likelihood\n",
    "    essay = open(files[i]).read() # Machine generated\n",
    "    input_tokens = tokenizer(essay, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    labels        = torch.clone(input_tokens)\n",
    "    labels[0,0]   = -100\n",
    "    output_prompt = model(input_tokens, labels=input_tokens)\n",
    "    nll_output = output_prompt.loss * input_tokens.size(1)\n",
    "    exp_features.append(nll_output)\n",
    "            \n",
    "    final_test_exp_features.append(exp_features)\n",
    "    final_test_labels.append(labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "base = LogisticRegression()\n",
    "model = CalibratedClassifierCV(base, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_train_exp_features)):\n",
    "    final_train_labels[i] = labels[train[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_test_exp_features)):\n",
    "    final_test_labels[i] = labels[test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(final_train_exp_features, final_train_labels)\n",
    "predictions = model.predict(final_test_exp_features[:])\n",
    "Accuracy = sum(torch.tensor(predictions.reshape((350))) == torch.tensor(final_test_labels))/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is 0.7028571367263794\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (Accuracy).item()\n",
    "print(\"Accuracy of the model is\", Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
